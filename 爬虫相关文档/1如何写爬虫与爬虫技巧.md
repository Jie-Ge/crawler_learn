### 爬虫技巧
1、Web爬取不了就爬取移动端网页

2、破解一家产品，成本从低到高思路如下：
  - 先看是否提供API
  - web端 < 公众号 < 小程序 < app(安卓) < app(IOS)
  - 在抓数据之前，首先看是否有web页面【比如访问链接（如抖音app有分享链接，可直接在web上访问）】
  - 在爬任何数据的时候，一定不要先考虑用Appium（效率低）

3、开始爬虫前正确的做法应该是：
  - 1.去百度和谷歌搜下这个网站有没有人分享出你要爬数据的API
  - 2.看看电脑网页有没有你要的数据，调查下好不好拿，不管好不好拿，也不要急着就开爬
  - 3.看看有没有电脑能打开的手机网站，一般格式为http://m.xxx.com或http://mobile.xxxx.com，有的话可以用F12检查抓下包，看下抓取难易程度
  - 4.看看有没有手机App，抓下App的包，看能不能抓到接口
  - 5.抓下公众号和小程序的包，看能不能抓到接口

3、抓取手机app时，参数是加密的，怎么办？
- 方法1：app逆向，找源码，找加密规则代码
- 方法2：利用mitmproxy，携带python脚本（编写代码：将返回的数据解析并写入本地文件）
  - 【参考】https://www.bilibili.com/video/BV1r541137ir?p=47&spm_id_from=pageDriver

### 写爬虫需要考虑的东西
- 1、去重
    - url去重、内容去重
- 2、效率、占用资源、费用
- 3、数据备份
    - 例如：每天做一次增量备份，每周做一次全量备份
- 4、爬虫监控
- 5、对于批量url，请求失败的url如何处理、记录
- 6、程序中途出错，如何保证每次启动都是爬剩下的而不是从头开始重复爬
- 7、请求失败得重试（可以是在 10 秒后重试，然后在 20 秒后重试，然后一分钟等等）

- 效率
- 并发量
- 超时时间
- 爬虫时间间隔
- 进入子程序的判断语句（如：if response.status == 200）
- **异常捕获**
- 面向对象
- 数据清洗

- 其他注意的地方
  - https://www.cnblogs.com/stlong/p/11223551.html
  - 每次启动都是爬剩下的而不是从头开始重复爬

### 踩过的坑
- 多线程
  - 使用多线程未处理异常，导致存活的线程越来越少，爬取的越来越慢
  - 多次保存数据到文件，导致速率慢
- 使用requests请求（或者其他模块请求），要加超时时间，否则会卡住

### 数据库
- 加入日期时间字段
