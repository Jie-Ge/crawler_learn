
## 爬虫工具清单.md 中也罗列了一些爬虫框架

## 1、scrapy
- 简介：爬虫框架
- 优点：可以高级定制化实现更加复杂的控制

### 2、pyspider（python3.6）
- 简介：爬虫框架，国内某大神开发的
- 优点：
  - 简单易上手，带图形界面（基于浏览器页面）
  - 简单的爬虫推荐使用，能同时维护几百个简单爬虫
- 缺点：
  - 可扩展性不强
  - 维护到18年，可能废弃了吧
  - python3.6及以下，坑较多
- 使用：
  - 【参考】https://blog.csdn.net/weixin_37947156/article/details/76495144
  - 1、pycharm控制台切到有pyspider的环境下，执行`pyspider`跑起来
      - 若想加载所有组建，执行 `pyspider all`
  - 2、可以看到webui运行在5000端口处，在浏览器打开 127.0.0.1:5000 或者 localhost:5000
  - PySpider 提供了动态解析 JS 的机制,  需要用到 PhantomJS，然后在代码中的 self.crawl()中添加参数 `fetch_type='js'`
  - 默认的代码含义：
```python
#!/usr/bin/env python
# -*- encoding: utf-8 -*-
# Created on 2021-11-29 11:55:32
# Project: reeoo
from pyspider.libs.base_handler import *
class Handler(BaseHandler):
    crawl_config = {
    }
    
    '''
     - POST 请求的URL是相同的，只是发送的参数不同，爬取第一页之后，后面的页数便不会再爬取。
        - 因为pyspider 以URL的 MD5 值作为唯一 ID 编号，ID编号相同，就视为同一个任务，不会再重复爬取。
     - 解决办法：需要重新写下 ID 编号的生成方式，在 on_start() 方法前面添加下面代码即可：
     def get_taskid(self,task):
        return md5string(task['url']+json.dumps(task['fetch'].get('data','')))
    '''
    
    @every(minutes=24 * 60)  # 通知 scheduler（框架的模块） 每天运行一次
    def on_start(self):  # 程序的入口
        self.crawl('https://reeoo.com/', callback=self.index_page)  # url 为爬取地址，callback 为抓取到数据response后的回调函数

    @config(age=10 * 24 * 60 * 60)  # 设置任务的有效期限，在这个期限内目标爬取的网页被认为不会进行修改
    def index_page(self, response):
        for each in response.doc('a[href^="http"]').items():  # response.doc 为 pyquery 对象，抓取对应标签的数据
            self.crawl(each.attr.href, callback=self.detail_page)
        '''
          1、response.json 用于解析json数据
          2、response.doc 返回的是PyQuery对象
          3、response.etree 返回的是lxml对象 （可用xpath解析）
          4、response.text 返回的是unicode文本
          5、response.content 返回的是字节码
        '''

    @config(priority=2)  # 设定任务优先级
    def detail_page(self, response):
      # 返回一个 dict 对象，结果会自动保存到默认的 result.db 中，也可以通过重写on_result(self, result)方法将数据存储到其他地方
      # 【注意】无论在哪个方法中return，都会将return的结果保存到 result.db 中
      # 除非是重写的方法，否则方法名是可以变的
        return {  
            "url": response.url,
            "title": response.doc('title').text(),
        }
    
    # 通过重写on_result(self, result)方法将数据存储到其他地方
    # def on_result(self, result):  # result是上面return的结果，所以要用此方法，那么之前就必须有return
    #     pass
```
  - pyspider爬取完毕后，再点击run是不会运行的。
    - 解决办法：在数据文件data中，保留 project.db 和 result.db, 删除其他文件即可。


## 3、Portia
- 简介：Portia是一个开源可视化爬虫工具，基于Scrapy，可让您在不需要任何编程知识的情况下可视化地爬取网站!
        简单地注释您感兴趣的页面，Portia将创建一个蜘蛛来从类似的页面提取数据
  
## 4、newspaper
- 【官方文档】https://newspaper.readthedocs.io/en/latest/user_guide/quickstart.html
- Newspaper库主要用于文章爬取和整理，国内的一个大佬做的。得到requests库的作者的盛赞。（一年多没维护）
- Newspaper特性：
    - 多线程文章下载框架
    - 新闻网址识别
    - 从html中提取文本、图像
    - 从文本中提取关键字
    - 从文本中提取摘要
    - 从文本中提取作者
    - Google趋势术语提取
    - 支持10种以上语言
- 优点
    - GitHub上点赞排名第三的爬虫框架
    - 适合抓取新闻网页
    - 操作非常简单易学
    - 整体的框架设计思路还是非常棒的
- 缺点
    - 使用它不需要考虑header、IP代理（不考虑这些会导致它访问网页时会有被直接拒绝的可能）
    - 不适用于实际工程类新闻信息爬取工作，
    - 框架不稳定，爬取过程中会有各种bug，例如获取不到url、新闻信息等
    - newspaper框架都是基于关键字识别的，有一些BUG存在，有时识别不准
- 使用
    - 安装：`pip install newspaper3k`
    - 100例 例80
    - 由于请求不需要设置参数，所以请求失败的机率很大，可以配合 requests使用，（requests爬取，newspaper充当解析器）
  
## 5、looter
- 【github地址】https://github.com/alphardex/looter
  【使用手册】https://looter.readthedocs.io/en/latest/
- 简介：国人写的小众框架（快两年没维护）


## 6、Ruia
- 【github地址】https://github.com/howie6879/ruia
- 简介：一款基于 asyncio 和 aiohttp 的异步爬虫框架
- 开发者所说的特点：easy、fast、extensible、powerful、写的少，跑得快
- 使用
    - 【参考】https://dream.blog.csdn.net/article/details/108361440
    - `pip install -U ruia`
