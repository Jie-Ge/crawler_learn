【https://baijiahao.baidu.com/s?id=1660689423524423511&wfr=spider&for=pc】
## 1、动态数据(Ajax、js动态渲染)

## 2、反爬
比方返回假数据、返回图片化数据、返回乱序数据、返回骂人的数据、返回求饶的数据；
反爬也得当心点，之前见过一个反爬直接返回 命令行 rm -rf 的也不是没有，你要是正好有个脚本模仿执行返回结果，结果就可以跑路了

## 3、浏览器模拟工具（自动化）
Puppeteer、Pyppeteer、Selenium、Splash

## 爬虫代码 或者 框架（如scrapy、pyspider）怎样部署

## 抓包工具
chales、fiddler、mitmproxy

## 4、并发
多进程、多线程、协程
异步协程，结合使用 aiohttp、gevent、tornado 等插件

## 5、避免网站封你 IP、封你账号、弹验证码、返回假数据

## 6、分布式爬虫
如今主流的 Python 散布式爬虫还是基于 Scrapy 的，对接 Scrapy-Redis、Scrapy-Redis-BloomFilter 或者用 Scrapy-Cluster 等等，
他们都是基于 Redis 来共享爬取队列的，总会多几少遇到一些内存的问题。所以一些人也思索对接到了其他的消息队列上，比方 RabbitMQ、Kafka 等等，其实效果也差不多少。
总之，想大规模、批量、高效的采集数据，分布式是必不可少的。

评估多少台机器合适


## 8、验证码识别

## 9、封 IP
云主机作为代理服务器；代理池

## 10、封账号
1、看看别的终端，比方手机页、App 页、wap 页，看看有没有能绕过登录的法子；
2、比较好的办法，那就是分流。假如你号足够多，建一个池子，比方 Cookies 池、Token 池、Sign 池反正不论什么池吧，
多个账号跑出来的 Cookies、Token 都放到这个池子里面，用的时分随机从里面拿一个。
假如你想保证爬取效率不变，那么 100 个账号相比 20 个账号，关于每个账号对应的 Cookies、Token 的取用频率就变成原来的了 1/5，那么被封的概率也就随之降低了。

## 11、JavaScript 逆向
用于解决加密，这一步是比较有难度的


## 12、App客户端
Appium、抓包工具（mitmproxy、Fiddler、Charles）
逆向、脱壳技术

## 13、数据库
msyql的增删改查、mongodb、redis。  
数据库调优、海量数据存储。  
数据保存肯定要会数据库的。不过有时候一些小数据也可以保存成json或者csv等。我有时想抓一些图片就直接按照文件夹保存文件。
推荐使用NoSQL的数据库，比如mongodb，因为爬虫抓到的数据一般是都字段-值得对应，有些字段有的网站有有的网站没有，mongo在这方面比较灵活，况且爬虫爬到的数据关系非常非常弱，很少会用到表与表的关系

## 14、HTTP知识/网络编程
- 【参考】：https://www.bilibili.com/video/BV1434y1U7B9?p=43&spm_id_from=pageDriver  
（url请求到返回的整个过程）
HTTP协议要理解。HTTP协议本身是无状态的，那么“登录”是怎么实现的？这就要求去了解一下session和cookies了。
GET方法和POST方法的区别（事实上除了字面意思不一样没有任何区别）。
浏览器要熟练。爬虫的过程其实是模拟人类去浏览器数据的过程。所以浏览器是怎么访问一个网站的，你要学会去观察，怎么观察呢？
Developer Tools！Chrome的Developer Tools提供了访问网站的一切信息。从traffic可以看到所有发出去的请求。
copy as curl功能可以给你生成和浏览器请求完全一致的curl请求！
我写一个爬虫的一般流程是这样的，先用浏览器访问，然后copy as curl看看有哪些header，cookies，然后用代码模拟出来这个请求，最后处理请求的结果保存下来

## 15、运维
爬虫的采集和更新策略
提升爬虫的性能（效率、质量）

## 16、数据的去重
url去重、内容去重（增量式爬虫）

## 17、爬虫框架
scrapy、pyspider、scrapyd等  
分布式爬虫框架：scrapy-redis、celery

## 18、爬虫的监控和部署

## 19、提高爬虫效率
- 多进程、多线程、异步协程、分布式、scrapy提高爬取速度
- Python建立数据库连接池提高效率

## 20、浏览器插件的使用
- chrome插件 Web Scraper 抓取数据

## 21、验证码提供平台
极验、网易易盾。  
网上应该有教程破解这些平台提供的验证码

## 22、云打码平台
超级鹰、百度ai

## 23、docker、部署容器化爬虫
简单地说，就是将你的项目和依赖包(基础镜像)打成一个带有启动指令的项目镜像，然后在服务器创建一个容器，让镜像在容器内运行，从而实现项目的部署。

## 其他：
- 1、要抓十几个相似度很高但是html结构不太一样的网站，我就写了一个简单的代码生成器，从爬虫代码到单元测试代码都可以自动生成，只要对应html结构稍微修改一下就行了。
- 2、我们需要一种任务队列，它的作用是：讲计划抓取的网页都放到任务队列里面去。然后worker从队列中拿出来一个一个执行，如果一个失败，记录一下，然后执行下一个。
  这样，worker就可以一个接一个地执行下去。也增加了扩展性，几亿个任务放在队列里也没问题，有需要可以增加worker，就像多一双亏筷子吃饭一样。
- 常用的任务队列有kafka，beanstalkd，celery等
- 多上github
- 网上觉得麻烦的，都可以考虑用爬虫解决（爬虫思维）
- 可以学习web知识。可视化用的是Django框架，表格展示数据用的是前端Datatable框架，各种图表展示用的是前端的echarts框架。数据分析方面，会Pandas库。
- 如何提升爬虫的性能（效率、质量）

## 如何成为高级爬虫工程师
- 【参考】：https://www.bilibili.com/video/BV1434y1U7B9?p=131
- 自动化部署和监控
- 应用相关
- 分布式存储
- 进一步解决反爬
- 进一步的解析方案
- 自己设计架构
- 多多的关注一些前端的开发技术，这样可以更好的了解到网站的基本结构，基本开发技巧，实现更优的爬虫手段

# 面试
- 爬过哪些网站
- 技术方面：
    - 1、反爬、代理、去重、增量、部署
        - 1）反反爬
            - 设置延时，合理控制速度
            - 伪装成浏览器，设置请求头
            - 设置cookie，登录状态
            - ip代理
            - 云打码平台处理验证码（）
            - js逆向解决加密、js反混淆
            - app逆向、脱壳
        - 2）代理
            - 构建ip代理池
    - 2、多线程、多进程；线程池、进程池
    - 3、HTML、CSS、JS、DOM、tcp/ip、ajax工作原理
        - DOM
            - DOM (Document Object Model) 译为文档对象模型，是 HTML 和 XML 文档的编程接口；
            - DOM 以树结构表达 HTML 文档，可以方便地访问、修改、添加和删除DOM树的结点和内容
        - tcp/ip协议（传输控制协议/网际协议）
            - 主要经历以下4个步骤：应用层，传输层，Internet层，物理层。
            - 假如你给你的基友发一个消息，数据开始传输，这时数据就要遵循TCP/IP协议啦，你的电脑会做出以下动作，这些动作你是看不到的。
            - 1）应用层先把你的消息进行格式转换,你的消息是文字还是图片，还是成人视频并进行加密等操作交给传输层。（这时的数据单元（单位）是信息）
            - 2）传输层将数据切割成一段一段的，便于传输并往里加上一些标记，比如当前应用的端口号等，交给Internet。（这时的数据单元（单位）是数据流）
            - 3）Internet层开始在将数据进行分组，分组头部包含目标地址的IP及一些相关信息交给物理层。（这时的数据单元（单位）是分组）
            - 4）物理层将数据转换为比特流开始查找主机真实物理地址进行校验等操作，校验通过，开始嗖~嗖~嗖~的住目的地跑。（这时的数据单元（单位）是比特）
            - 到达目的地后，对方设备会将上面的顺序反向的操作一遍，最后呈现出来。
        - ajax工作原理
            - Ajax（Asyncchronous JavaScript and Xml），翻译过来就是说：异步的javaScript和xml
            - 在不需要重新刷整个页面的情况下，Ajax 通过异步请求加载后台数据，并在网页上产生局部刷新的效果
              
            - Ajax的工作原理相当于在用户和服务器之间加了一个中间层(Ajax引擎)，使用户操作与服务器响应异步化。并不是所有用户请求都提交服务器。
              像一些数据验证（表单验证是否登入成功）和数据处理等都交给Ajax引擎自己来做，只有确定需要从服务器读取新数据时再由Ajax引擎代为向服务器提交请求
              
            - Ajax与服务器通信有一个核心的对象 XMLHttpRequest
            - 例如视频播放网站，首次播放时（静态内容与动态内容分别请求加载）会加载整个页面，很多信息，很多请求（html文档、图片、js脚本、音乐、广告），
              再点击下一集时只有视频播放区域局部刷新，就不需要再加载这些静态内容了
    - 4、爬虫库
        - requests
    - 5、数据提取
        - 正则表达式、xpath提取页面元素（或者bs4、pyquery）
    - 6、框架
        - scrapy
    - 7、数据库
        - msyql的增删改查、mongodb、redis
    - 8、linux
        - 熟悉Linux日常工作环境，熟练掌握常用命令和调优监控手段
    - 9、git
        - 团队协作开发工具git的熟练使用（版本控制）
    - 10、数据清洗、数据分析
        - python科学计算库numpy、scipy和数据分析库pandas的熟练使用
    - 11、熟悉http、https原理，理解cookie机制
    - 12、文本、图片、视频
    - 13、抓包工具
- 项目经历
- 离职理由
- 背调
    - 于洲 18780062818
    - 曾叙平 15082386093
    - 汤浩 18615789588
    - 蔡英珏 13438281560
    - 陈凤凯 15228274892
# 职场
- 多问多交流，主动找事做
- 不只要埋头苦干，也要让领导看到你的发光点
- 多做笔记
- 明白需求 -》可行性分析 -》理性思路 -》干
- 考虑公司的利益、费用（爬虫支出）