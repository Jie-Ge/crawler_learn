### 1、queue
- 包中的常用方法:
    - queue.qsize() 返回队列的大小
    - queue.empty() 如果队列为空，返回True,反之False
    - queue.full() 如果队列满了，返回True,反之False
    - queue.full 与 maxsize 大小对应
    - put(item[, block[, timeout]])
      - 如果可选的参数block为True且timeout为空对象（默认的情况，阻塞调用，无超时）。
      - 如果timeout是个正整数，阻塞调用进程最多timeout秒，如果一直无空空间可用，抛出Full异常（带超时的阻塞调用）。
      - 如果block为False，如果有空闲空间可用将数据放入队列，否则立即抛出Full异常
    - queue.get([block[, timeout]]) 
      - 同上

- 创建一个“队列”对象
  - import queue
  - myqueue = queue.Queue(maxsize = 10)

- 将一个值放入队列中
  - myqueue.put(10)

- 将一个值从队列中取出
  - myqueue.get()
  
## 一、请求解析

### 1.1、request
- response.text 返回的是Unicode格式，通常需要转换为utf-8格式，否则就是乱码。
- response.content 是二进制模式，可以下载视频之类的，如果想看的话需要decode成utf-8格式

### 1.2、retrying
- 【参考】官方文档：https://pypi.org/project/retrying/
- 介绍：最简单的使用就是给你想不断重试的方法加上 装饰器 @retry
    - 例如：我希望网络请求模块尝试3次之后，在报错！
```python
from retrying import retry
import requests

@retry
def get_html():
    url = ''
    req = requests.get(url)

@retry(stop_max_attempt_number=3)
def stop_after_7_attempts():
    print("Stopping after 3 attempts")
```

### 1.3、requests-html
- 【参考】https://www.cnblogs.com/abdm-989/p/12143473.html
- 1、简介
    - requests-html只支持Python 3.6或以上的版本
    - 具备requests的功能以外，还新增了一些更加强大的功能
    - requests获取响应数据，接着再利用bs4或xpath解析库；而在requests-html里面只需要一步就可以完成，而且可以直接进行js渲染
- 2、使用
- 2.1 获取链接
```python
from requests_html import HTMLSession
# 获取请求对象
session = HTMLSession()

# 往新浪新闻主页发送get请求
sina = session.get('https://news.sina.com.cn/')
# print(sina.status_code)
sina.encoding = 'utf-8'

# 获取响应文本信息，与requests无区别
# print(sina.text)

## 获取链接
# 得到新浪新闻主页所有的链接，返回的是一个set集合
print(sina.html.links)
print('*' * 1000)

# 若获取的链接中有相对路径，我们还可以通过absolute_links获取所有绝对链接
print(sina.html.absolute_links)

## 解析数据
# 使用xpath解析数据
hrefs = sina.html.xpath("//p[@class='media-heading lead']/a/@href")
# 使用find解析数据
href = sina.html.find('h3>a',first=True).attrs["href"]
course_target = sina.html.find(".main>.course_target", first=True).text
```

### 1.4、pyquery
- 1、简介：
    - PyQuery是一个类似于jQuery的解析网页工具
    - pyquery的强大之处就在于它有强大的CSS选择器
    - 和XPATH，BeautifulSoup比起来，PyQuery更加灵活，提供增加节点的class属性，移除某个节点，添加文本信息等功能
    
- 2、基本使用
  - 可以传入字符串、URL、文件名等HTML文本
- 2.1 传入URL
```python
from pyquery import PyQuery as pq
doc = pq(url='https://movie.douban.com/cinema/nowplaying/chengdu/')
print(doc('title'))
# PyQuery对象会首先请求这个url，然后用得到的html内容完成初始化
## 同下面
from pyquery import PyQuery as pq
import requests
text = requests.get('https://movie.douban.com/cinema/nowplaying/chengdu/').text
doc = pq(text)
print(doc('title'))
```
- 2.2 传入字符串型的html文本
```python
from pyquery import PyQuery as pq

s = '<html><title class="name"><p>PyQuery用法总结</p><title></html>'
doc = pq(s)
print(doc('title'))
```
- 2.3 传入html文件
```python
from pyquery import PyQuery as pq
doc = pq(filename='rr.html')
print(doc('img'))
```
- 2.4 选取节点
```python
print(doc('#items .list li'))
doc.find('.name>p').text()
doc.find('html p').text()
```

## 二、自动化测试工具

### 2.1、pyppeteer
- 1、简介
  - 同 selenium 是一款web自动化测试工具
  - Pyppeteer 其实是 Puppeteer 的 Python 版本, Puppeteer 是Google基于Node.js开发的一个工具，主要是用来操纵Chrome浏览器的API
- 2、优点
  - 安装配置的便利性(运行Pyppeteer会自动安装驱动) 和 运行效率(基于asyncio，支持异步操作)方面都要远胜 selenium (有的网站会对selenium和webdriver进行识别和反爬)
  - 具有异步加载、速度快、具备有界面/无界面模式、伪装性更强不易被识别为机器人同时可以伪装手机平板等终端
- 3、缺点
  - 支持的浏览器比较单一（only Chrome）
  - 本来 chrome 就问题多多，puppeteer 也是各种坑，加上 pyppeteer 是基于前者的改编 python 版本，也就是产生了只要前两个有一个有 bug，
    那么 pyppeteer 就会原封不动的继承下来
  - pyppeteer维护的不好，导致很多 bug
- 4、使用
- 官方文档：https://pyppeteer.github.io/pyppeteer/reference.html
```python
import asyncio
from pyppeteer import launch
from lxml import etree

async def main():
    browser = await launch(headless=False,args=['--disable-infobars'])  # 运行一个无头的浏览器,headless是否输出网页源码
    
    # 打开新的标签页
    page = await browser.newPage()

    #设置视图大小
    await page.setViewport({'width':1366,'height':768})

    #设置UserAgent
    await page.setUserAgent('Mozilla/5.0 (Windows NT 6.1; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.77 Safari/537.36')

    # 访问页面
    response = await page.goto('https://www.baidu.com')
    
    await page.screenshot({'path': 'baidu.png'})  # 把网页生成截图
    
    # status
    print(response.status)
    #获取当前页内容
    print(await page.content()) #文本类型
    # print(await response.text())
    
    #cookie操作
    print(await page.cookies()) #获取cookie,[{'name':xx,'value':xxx...},...]
    # page.deleteCookie() 删除cookie
    # page.setCookie() 设置cookie
    
    #定位元素 （返回的是ElementHandle类型，若闲麻烦，可以用etree.HTML)
    #1、只定位一个元素（css选择器）
    # element = await page.querySelector('#s-top-left > a')
    #2、css选择器
    elements = await page.querySelectorAll('#s-top-left > a:nth-child(2n)')
    #3、xpath
    # elements = await page.xpath('//div[@id="s-top-left"]/a')
    for element in elements:
        print(await (await element.getProperty('textContent')).jsonValue()) #获取文本内容
        print(await (await element.getProperty('href')).jsonValue())#获取href属性
    
    #4、xpath
    # doc = etree.HTML(await page.content())
    # doc.xpath('')

    #模拟输入和点击
    await page.type('#kw','中国',{'delay':1000}) #模拟输入，输入时间:1000 ms
    await asyncio.sleep(2)
    await page.click('#su') #模拟点击，也可以先定位元素，然后await element.click()
    await asyncio.sleep(2)

    #执行js，滚动页面到底部
    await page.evaluate('window.scrollTo(0,document.body.scrollHeight);')

    await asyncio.sleep(5)
    await browser.close()

asyncio.get_event_loop().run_until_complete(main())  # 异步
# asyncio.run(main())
```

### 2.2 selenium
- 在 selenium_.py 中

### 2.3 playwright
- 1、简介：Playwright是由微软公司2020年初发布的新一代自动化测试工具，Playwright 是针对 Python 语言的纯自动化工具，
  相较于目前最常用的Selenium，它仅用一个API即可自动执行Chromium、Firefox、WebKit等主流浏览器自动化操作，同时支持以无头模式、有头模式运行。
- 2、优点
  - 支持浏览器端的录制，生成自动化脚本，支持无头跑脚本
  - 速度快，基本是selenium的好几倍，且支持浏览器异步运行
  - 自动等待API，可拦截请求，随意模拟
  - 自动等待API。Playwright交互会自动等待直到元素准备就绪（比如点击事件，会自动等待元素加载完成再点击）
- 3、缺点
  - 目前执行报错还是一堆，一堆坑
  - Playwright不支持旧版Microsoft Edge或IE11。支持新的Microsoft Edge（在Chromium上）；所以对浏览器版本有硬性要求的项目不适用
  - 需要SSL证书进行访问的网站可能无法录制，该过程需要单独定位编写
  - 移动端测试是通过桌面浏览器来模拟移动设备（相当于自带模拟器），无法控制真机
- 4、使用



## 三、爬虫框架

### 3.1、pyspider（python3.6）
- 简介：爬虫框架，国内某大神开发的
- 优点：
  - 简单易上手，带图形界面（基于浏览器页面）
  - 简单的爬虫推荐使用，能同时维护几百个简单爬虫
- 缺点：
  - 维护到18年，可能废弃了吧
  - python3.6及以下，坑较多
- 使用：
  - 【参考】https://blog.csdn.net/weixin_37947156/article/details/76495144
  - 1、pycharm控制台切到有pyspider的环境下，执行`pyspider`跑起来
      - 若想加载所有组建，执行 `pyspider all`
  - 2、可以看到webui运行在5000端口处，在浏览器打开 127.0.0.1:5000 或者 localhost:5000
  - PySpider 提供了动态解析 JS 的机制,  需要用到 PhantomJS，然后在代码中的 self.crawl()中添加参数 `fetch_type='js'`
  - 默认的代码含义：
```python
#!/usr/bin/env python
# -*- encoding: utf-8 -*-
# Created on 2021-11-29 11:55:32
# Project: reeoo
from pyspider.libs.base_handler import *
class Handler(BaseHandler):
    crawl_config = {
    }
    
    '''
     - POST 请求的URL是相同的，只是发送的参数不同，爬取第一页之后，后面的页数便不会再爬取。
        - 因为pyspider 以URL的 MD5 值作为唯一 ID 编号，ID编号相同，就视为同一个任务，不会再重复爬取。
     - 解决办法：需要重新写下 ID 编号的生成方式，在 on_start() 方法前面添加下面代码即可：
     def get_taskid(self,task):
        return md5string(task['url']+json.dumps(task['fetch'].get('data','')))
    '''
    
    @every(minutes=24 * 60)  # 通知 scheduler（框架的模块） 每天运行一次
    def on_start(self):  # 程序的入口
        self.crawl('https://reeoo.com/', callback=self.index_page)  # url 为爬取地址，callback 为抓取到数据response后的回调函数

    @config(age=10 * 24 * 60 * 60)  # 设置任务的有效期限，在这个期限内目标爬取的网页被认为不会进行修改
    def index_page(self, response):
        for each in response.doc('a[href^="http"]').items():  # response.doc 为 pyquery 对象，抓取对应标签的数据
            self.crawl(each.attr.href, callback=self.detail_page)
        '''
          1、response.json 用于解析json数据
          2、response.doc 返回的是PyQuery对象
          3、response.etree 返回的是lxml对象 （可用xpath解析）
          4、response.text 返回的是unicode文本
          5、response.content 返回的是字节码
        '''

    @config(priority=2)  # 设定任务优先级
    def detail_page(self, response):
      # 返回一个 dict 对象，结果会自动保存到默认的 result.db 中，也可以通过重写on_result(self, result)方法将数据存储到其他地方
      # 【注意】无论在哪个方法中return，都会将return的结果保存到 result.db 中
      # 除非是重写的方法，否则方法名是可以变的
        return {  
            "url": response.url,
            "title": response.doc('title').text(),
        }
    
    # 通过重写on_result(self, result)方法将数据存储到其他地方
    # def on_result(self, result):  # result是上面return的结果，所以要用此方法，那么之前就必须有return
    #     pass
```
  - pyspider爬取完毕后，再点击run是不会运行的。
    - 解决办法：在数据文件data中，保留 project.db 和 result.db, 删除其他文件即可。
  
### 3.2、scrapy
- 简介：爬虫框架
- 优点：可以高级定制化实现更加复杂的控制
