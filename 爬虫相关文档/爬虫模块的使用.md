## 1、request
- response.text 返回的是Unicode格式，通常需要转换为utf-8格式，否则就是乱码。
- response.content 是二进制模式，可以下载视频之类的，如果想看的话需要decode成utf-8格式

## 2、retrying
- 【参考】官方文档：https://pypi.org/project/retrying/
- 介绍：最简单的使用就是给你想不断重试的方法加上 装饰器 @retry
    - 例如：我希望网络请求模块尝试3次之后，在报错！
```python
from retrying import retry
import requests

@retry
def get_html():
    url = ''
    req = requests.get(url)

@retry(stop_max_attempt_number=3)
def stop_after_7_attempts():
    print("Stopping after 3 attempts")
```

## 3、queue
- 包中的常用方法:
    - queue.qsize() 返回队列的大小
    - queue.empty() 如果队列为空，返回True,反之False
    - queue.full() 如果队列满了，返回True,反之False
    - queue.full 与 maxsize 大小对应
    - put(item[, block[, timeout]])
      - 如果可选的参数block为True且timeout为空对象（默认的情况，阻塞调用，无超时）。
      - 如果timeout是个正整数，阻塞调用进程最多timeout秒，如果一直无空空间可用，抛出Full异常（带超时的阻塞调用）。
      - 如果block为False，如果有空闲空间可用将数据放入队列，否则立即抛出Full异常
    - queue.get([block[, timeout]]) 
      - 同上

- 创建一个“队列”对象
  - import queue
  - myqueue = queue.Queue(maxsize = 10)

- 将一个值放入队列中
  - myqueue.put(10)

- 将一个值从队列中取出
  - myqueue.get()

## 4、requests-html
- 【参考】https://www.cnblogs.com/abdm-989/p/12143473.html
- 1、简介
    - requests-html只支持Python 3.6或以上的版本
    - 具备requests的功能以外，还新增了一些更加强大的功能
    - requests获取响应数据，接着再利用bs4或xpath解析库；而在requests-html里面只需要一步就可以完成，而且可以直接进行js渲染
- 2、使用
- 2.1 获取链接
```python
from requests_html import HTMLSession
# 获取请求对象
session = HTMLSession()

# 往新浪新闻主页发送get请求
sina = session.get('https://news.sina.com.cn/')
# print(sina.status_code)
sina.encoding = 'utf-8'

# 获取响应文本信息，与requests无区别
# print(sina.text)

## 获取链接
# 得到新浪新闻主页所有的链接，返回的是一个set集合
print(sina.html.links)
print('*' * 1000)

# 若获取的链接中有相对路径，我们还可以通过absolute_links获取所有绝对链接
print(sina.html.absolute_links)

## 解析数据
# 使用xpath解析数据
hrefs = sina.html.xpath("//p[@class='media-heading lead']/a/@href")
# 使用find解析数据
href = sina.html.find('h3>a',first=True).attrs["href"]
course_target = sina.html.find(".main>.course_target", first=True).text
```

## 5、pyquery
- 1、简介：
    - PyQuery是一个类似于jQuery的解析网页工具
    - pyquery的强大之处就在于它有强大的CSS选择器
    - 和XPATH，BeautifulSoup比起来，PyQuery更加灵活，提供增加节点的class属性，移除某个节点，添加文本信息等功能
    
- 2、基本使用
  - 可以传入字符串、URL、文件名等HTML文本
- 2.1 传入URL
```python
from pyquery import PyQuery as pq
doc = pq(url='https://movie.douban.com/cinema/nowplaying/chengdu/')
print(doc('title'))
# PyQuery对象会首先请求这个url，然后用得到的html内容完成初始化
## 同下面
from pyquery import PyQuery as pq
import requests
text = requests.get('https://movie.douban.com/cinema/nowplaying/chengdu/').text
doc = pq(text)
print(doc('title'))
```
- 2.2 传入字符串型的html文本
```python
from pyquery import PyQuery as pq

s = '<html><title class="name"><p>PyQuery用法总结</p><title></html>'
doc = pq(s)
print(doc('title'))
```
- 2.3 传入html文件
```python
from pyquery import PyQuery as pq
doc = pq(filename='rr.html')
print(doc('img'))
```
- 2.4 选取节点
```python
print(doc('#items .list li'))
doc.find('.name>p').text()
doc.find('html p').text()
```

## 6、pyppeteer
- 1、简介
  - 同 selenium 是一款web自动化测试工具
  - Pyppeteer 其实是 Puppeteer 的 Python 版本, Puppeteer 是Google基于Node.js开发的一个工具，主要是用来操纵Chrome浏览器的API
- 2、优点
  - 安装配置的便利性(运行Pyppeteer会自动安装驱动) 和 运行效率(基于asyncio，支持异步操作)方面都要远胜 selenium (有的网站会对selenium和webdriver进行识别和反爬)
- 3、缺点
  - 支持的浏览器比较单一（only Chrome）
  - 本来 chrome 就问题多多，puppeteer 也是各种坑，加上 pyppeteer 是基于前者的改编 python 版本，也就是产生了只要前两个有一个有 bug，
    那么 pyppeteer 就会原封不动的继承下来，本来这没什么
  - pyppeteer维护的不好，导致很多 bug
- 4、使用
- 官方文档：https://pyppeteer.github.io/pyppeteer/reference.html
```python
import asyncio
from pyppeteer import launch
from lxml import etree

async def main():
    browser = await launch(headless=False,args=['--disable-infobars'])  # 运行一个无头的浏览器,headless是否输出网页源码
    
    # 打开新的标签页
    page = await browser.newPage()

    #设置视图大小
    await page.setViewport({'width':1366,'height':768})

    #设置UserAgent
    await page.setUserAgent('Mozilla/5.0 (Windows NT 6.1; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.77 Safari/537.36')

    # 访问页面
    response = await page.goto('https://www.baidu.com')
    
    await page.screenshot({'path': 'baidu.png'})  # 把网页生成截图
    
    # status
    print(response.status)
    #获取当前页内容
    print(await page.content()) #文本类型
    # print(await response.text())
    
    #cookie操作
    print(await page.cookies()) #获取cookie,[{'name':xx,'value':xxx...},...]
    # page.deleteCookie() 删除cookie
    # page.setCookie() 设置cookie
    
    #定位元素 （返回的是ElementHandle类型，若闲麻烦，可以用etree.HTML)
    #1、只定位一个元素（css选择器）
    # element = await page.querySelector('#s-top-left > a')
    #2、css选择器
    elements = await page.querySelectorAll('#s-top-left > a:nth-child(2n)')
    #3、xpath
    # elements = await page.xpath('//div[@id="s-top-left"]/a')
    for element in elements:
        print(await (await element.getProperty('textContent')).jsonValue()) #获取文本内容
        print(await (await element.getProperty('href')).jsonValue())#获取href属性
    
    #4、xpath
    # doc = etree.HTML(await page.content())
    # doc.xpath('')

    #模拟输入和点击
    await page.type('#kw','中国',{'delay':1000}) #模拟输入，输入时间:1000 ms
    await asyncio.sleep(2)
    await page.click('#su') #模拟点击，也可以先定位元素，然后await element.click()
    await asyncio.sleep(2)

    #执行js，滚动页面到底部
    await page.evaluate('window.scrollTo(0,document.body.scrollHeight);')

    await asyncio.sleep(5)
    await browser.close()

asyncio.get_event_loop().run_until_complete(main())  # 异步
# asyncio.run(main())
```