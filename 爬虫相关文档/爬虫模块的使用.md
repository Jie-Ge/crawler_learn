### 1、queue
- 包中的常用方法:
    - queue.qsize() 返回队列的大小
    - queue.empty() 如果队列为空，返回True,反之False
    - queue.full() 如果队列满了，返回True,反之False
    - queue.full 与 maxsize 大小对应
    - put(item[, block[, timeout]])
      - 如果可选的参数block为True且timeout为空对象（默认的情况，阻塞调用，无超时）。
      - 如果timeout是个正整数，阻塞调用进程最多timeout秒，如果一直无空空间可用，抛出Full异常（带超时的阻塞调用）。
      - 如果block为False，如果有空闲空间可用将数据放入队列，否则立即抛出Full异常
    - queue.get([block[, timeout]]) 
      - 同上

- 创建一个“队列”对象
  - import queue
  - myqueue = queue.Queue(maxsize = 10)

- 将一个值放入队列中
  - myqueue.put(10)

- 将一个值从队列中取出
  - myqueue.get()

### 2、execjs
- 简介：python中执行js脚本的模块
- 安装：`pip install PyExecJS`
- 使用：
```python
import execjs #导入

with open('js_code.js',encoding='utf-8') as f:
    js_code = f.read()

# 编译js代码
ctx = execjs.compile(js_code)

# 两种方式执行js函数
data1 = ctx.eval('getParam({"city":"北京"})')  # eval方法中，整个函数调用包含在字符串内
data2 = ctx.call('getParam',{'city':'北京'})  # call方法中，第一个参数是函数名（str）,后面接参数
```

  
## 一、请求解析

### 1.1、request
- response.text 返回的是Unicode格式，通常需要转换为utf-8格式，否则就是乱码。
- response.content 是二进制模式，可以下载视频之类的，如果想看的话需要decode成utf-8格式

### 1.2、retrying
- 【参考】官方文档：https://pypi.org/project/retrying/
- 介绍：最简单的使用就是给你想不断重试的方法加上 装饰器 @retry
    - 例如：我希望网络请求模块尝试3次之后，在报错！
```python
from retrying import retry
import requests

@retry
def get_html():
    url = ''
    req = requests.get(url)

@retry(stop_max_attempt_number=3)
def stop_after_7_attempts():
    print("Stopping after 3 attempts")
```

### 1.3、requests-html
- 【参考】https://www.cnblogs.com/abdm-989/p/12143473.html
- 1、简介
    - requests-html只支持Python 3.6或以上的版本
    - 具备requests的功能以外，还新增了一些更加强大的功能
    - requests获取响应数据，接着再利用bs4或xpath解析库；而在requests-html里面只需要一步就可以完成，而且可以直接进行js渲染
- 2、使用
- 2.1 获取链接
```python
from requests_html import HTMLSession
# 获取请求对象
session = HTMLSession()

# 往新浪新闻主页发送get请求
sina = session.get('https://news.sina.com.cn/')
# print(sina.status_code)
sina.encoding = 'utf-8'

# 获取响应文本信息，与requests无区别
# print(sina.text)

## 获取链接
# 得到新浪新闻主页所有的链接，返回的是一个set集合
print(sina.html.links)
print('*' * 1000)

# 若获取的链接中有相对路径，我们还可以通过absolute_links获取所有绝对链接
print(sina.html.absolute_links)

## 解析数据
# 使用xpath解析数据
hrefs = sina.html.xpath("//p[@class='media-heading lead']/a/@href")
# 使用find解析数据
href = sina.html.find('h3>a',first=True).attrs["href"]
course_target = sina.html.find(".main>.course_target", first=True).text
```

### 1.4、pyquery
- 1、简介：
    - PyQuery是一个类似于jQuery的解析网页工具
    - pyquery的强大之处就在于它有强大的CSS选择器
    - 和XPATH，BeautifulSoup比起来，PyQuery更加灵活，提供增加节点的class属性，移除某个节点，添加文本信息等功能
    
- 2、基本使用
  - 可以传入字符串、URL、文件名等HTML文本
- 2.1 传入URL
```python
from pyquery import PyQuery as pq
doc = pq(url='https://movie.douban.com/cinema/nowplaying/chengdu/')
print(doc('title'))
# PyQuery对象会首先请求这个url，然后用得到的html内容完成初始化
## 同下面
from pyquery import PyQuery as pq
import requests
text = requests.get('https://movie.douban.com/cinema/nowplaying/chengdu/').text
doc = pq(text)
print(doc('title'))
```
- 2.2 传入字符串型的html文本
```python
from pyquery import PyQuery as pq

s = '<html><title class="name"><p>PyQuery用法总结</p><title></html>'
doc = pq(s)
print(doc('title'))
```
- 2.3 传入html文件
```python
from pyquery import PyQuery as pq
doc = pq(filename='rr.html')
print(doc('img'))
```
- 2.4 选取节点
```python
print(doc('#items .list li'))
doc.find('.name>p').text()
doc.find('html p').text()
```

### 1.5、pandas.read_html()
- 简介：此方法用于获取网页中的表格（也只能获取表格）
- 方法特点：
  - 只能获取表格
  - 不能读取 https
  - 不能防止反爬，可配合其他请求网页的模块使用
- 使用：
  - 100例 例91

### 1.6、wget
- 简介：wget是个专职的下载利器,简单,专一,极致，用于从Web上进行非交互式文件下载，即使用户没有登录也可以在后台运行
- 安装： `pip install wget`
- 使用：
  - 可结合mitmproxy使用【https://zhuanlan.zhihu.com/p/50185975】

## 二、自动化测试工具

### 2.1、pyppeteer
- 1、简介
  - 同 selenium 是一款web自动化测试工具
  - Pyppeteer 其实是 Puppeteer 的 Python 版本, Puppeteer 是Google基于Node.js开发的一个工具，主要是用来操纵Chrome浏览器的API
- 2、优点
  - 安装配置的便利性(运行Pyppeteer会自动安装驱动) 和 运行效率(基于asyncio，支持异步操作)方面都要远胜 selenium (有的网站会对selenium和webdriver进行识别和反爬)
  - 具有异步加载、速度快、具备有界面/无界面模式、伪装性更强不易被识别为机器人，同时可以伪装手机平板等终端
- 3、缺点
  - 支持的浏览器比较单一（only Chrome）
  - 本来 chrome 就问题多多，puppeteer 也是各种坑，加上 pyppeteer 是基于前者的改编 python 版本，也就是产生了只要前两个有一个有 bug，
    那么 pyppeteer 就会原封不动的继承下来
  - 好消息是又开始维护了，那么pyppeteer
- 4、使用
- 官方文档：https://pyppeteer.github.io/pyppeteer/reference.html
```python
import asyncio
from pyppeteer import launch
from lxml import etree

async def main():
    browser = await launch(headless=False,args=['--disable-infobars'])  # 运行一个无头的浏览器,headless是否输出网页源码
    
    # 打开新的标签页
    page = await browser.newPage()

    #设置视图大小
    await page.setViewport({'width':1366,'height':768})

    #设置UserAgent
    await page.setUserAgent('Mozilla/5.0 (Windows NT 6.1; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.77 Safari/537.36')

    # 访问页面
    response = await page.goto('https://www.baidu.com')
    
    await page.screenshot({'path': 'baidu.png'})  # 把网页生成截图
    
    # status
    print(response.status)
    #获取当前页内容
    print(await page.content()) #文本类型
    # print(await response.text())
    
    #cookie操作
    print(await page.cookies()) #获取cookie,[{'name':xx,'value':xxx...},...]
    # page.deleteCookie() 删除cookie
    # page.setCookie() 设置cookie
    
    #定位元素 （返回的是ElementHandle类型，若闲麻烦，可以用etree.HTML)
    #1、只定位一个元素（css选择器）
    # element = await page.querySelector('#s-top-left > a')
    #2、css选择器
    elements = await page.querySelectorAll('#s-top-left > a:nth-child(2n)')
    #3、xpath
    # elements = await page.xpath('//div[@id="s-top-left"]/a')
    for element in elements:
        print(await (await element.getProperty('textContent')).jsonValue()) #获取文本内容
        print(await (await element.getProperty('href')).jsonValue())#获取href属性
    
    #4、xpath
    # doc = etree.HTML(await page.content())
    # doc.xpath('')

    #模拟输入和点击
    await page.type('#kw','中国',{'delay':1000}) #模拟输入，输入时间:1000 ms
    await asyncio.sleep(2)
    await page.click('#su') #模拟点击，也可以先定位元素，然后await element.click()
    await asyncio.sleep(2)

    #执行js，滚动页面到底部
    await page.evaluate('window.scrollTo(0,document.body.scrollHeight);')

    await asyncio.sleep(5)
    await browser.close()

asyncio.get_event_loop().run_until_complete(main())  # 异步
# asyncio.run(main())
```

### 2.2 selenium
- 在 selenium_.py 中

### 2.3 playwright
- 1、简介：Playwright是由微软公司2020年初发布的新一代自动化测试工具，Playwright 是针对 Python 语言的纯自动化工具，
  相较于目前最常用的Selenium，它仅用一个API即可自动执行Chromium、Firefox、WebKit等主流浏览器自动化操作，同时支持以无头模式、有头模式运行。
- 2、优点
  - 支持浏览器端的录制，生成自动化脚本，支持无头跑脚本
  - 速度快，基本是selenium的好几倍，且支持浏览器异步运行
  - 自动等待API，可拦截请求，随意模拟
  - 自动等待API。Playwright交互会自动等待直到元素准备就绪（比如点击事件，会自动等待元素加载完成再点击）
- 3、缺点
  - 目前执行报错还是一堆，一堆坑
  - Playwright不支持旧版Microsoft Edge或IE11。支持新的Microsoft Edge（在Chromium上）；所以对浏览器版本有硬性要求的项目不适用
  - 需要SSL证书进行访问的网站可能无法录制，该过程需要单独定位编写
  - 移动端测试是通过桌面浏览器来模拟移动设备（相当于自带模拟器），无法控制真机
- 4、使用


## 三、验证码
### 3.1 pytesseract
- 简介
  - 专门用于对图片文字进行识别，并获取文本。但是它的缺点是对手写的识别能力比较差
  
- 特点
  - 只能识别简单的验证码（简单的验证码就是字符跟字符之间没有粘在一起，然后角度都是正的，没有其他杂质干扰）
  - 需要比较规范的字体，在实际运用中效果差
  - 功能待熟悉，文档中应该有方法处理带干扰的图片，需要自己去读文档
  
- 安装
  - 1）安装模块
    - `pip install pytesseract`
  - 2）tesseract-OCR 安装
    - 【Mac参考】https://www.cnblogs.com/cheflone/articles/13895619.html
    - 【Win参考】https://dream.blog.csdn.net/article/details/88411641
  - 3）语言库下载
    - 上一步下载安装后可能没有 chi_sim.traineddata（简体中文）语言库，需要手动下载
    - https://github.com/tesseract-ocr/tessdata 中只下载 chi_sim.traineddata
    - 将chi_sim.traineddata下载后，需要将它放在 share 目录下
      - 【Mac】/usr/local/Cellar/tesseract/4.1.1/share/tessdata 目录下
      - 【Win】...
  - 4）修改文件 pytesseract.py
    - 文件中 `tesseract_cmd = 'tesseract'` 改为自己的地址
      - 【Mac】`tesseract_cmd = '/opt/homebrew/Cellar/tesseract/4.1.1/bin/tesseract'`
      - 【Win】`...\\tesseract.exe`

- 使用
  - 见100例
  - 以及 https://dream.blog.csdn.net/article/details/88411641
